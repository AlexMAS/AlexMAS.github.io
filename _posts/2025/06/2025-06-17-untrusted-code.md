---
layout: post
title: Безопасное исполнение ненадёжного кода
date: 2025-06-17
tags: conf dev arch untrusted_code
categories: article
thumbnail: /assets/img/blog/2025/2025-06-17-untrusted-code.jpg
toc:
  beginning: true
metadata:
  selection: arch-and-dev
---

Мы привыкли к тому, что ведем разработку, используя лучшие инженерные практики, включая настройку CI/CD-конвейера. Сначала код проходит многоэтапные стадии проверки и тестирования, а только потом попадает в production-среду. Давайте представим ситуацию, что нужно запустить код, минуя все эти стадии. Прям в production-среде. На первый взгляд — бред! Но если подумать, то на самом деле не такая уж редкость. Например, некоторые системы предоставляют своим пользователям возможность расширять функциональность за счет прикладных скриптов. Наш любимый CI/CD-конвейер зачастую построен на пользовательских скриптах.

![](/assets/img/blog/2025/2025-06-17-untrusted-code.jpg)

С одной стороны, для большинства подобная постановка вопроса является крайностью. С другой, появляется возможность рассмотреть проблему с разных ракурсов. Уверен, что какие-то части общего решения, о котором пойдёт речь далее, могут быть использованы повторно и в других проектах.

Ниже я рассмотрю проблему безопасного исполнения ненадёжного кода с разных сторон. Последовательно рассмотрю вопросы, ответы на которые являются поворотными в выборе целевой архитектуры. Большая часть статьи касается разработки, но в конце сделаны важные акценты относительно администрирования и развертывания.

> **Небольшая предыстория**
> <br/><br/>
> Статья написана на основе опыта, полученного в ходе разработки и эксплуатации системы для проведения онлайн-олимпиад по программированию. В общих чертах задача заключается в потоковой автоматической проверке присылаемых решений, которые оформляются в виде кода программы на каком-то языке программирования. Проверка предполагает многократное исполнение полученного кода на предопределенном наборе тестовых данных.
> <br/><br/>
> В моменты проведения олимпиад система проверяет до миллиона решений в час. Это серьёзная нагрузка и испытание, учитывая, что прислать могут всё, что угодно. По статистике, примерно 20% присылаемых решений — вредоносный или опасный для исполнения код, который может нанести вред системе или инфраструктуре.
> <br/><br/>
> Для тех, кому интересны детали данного проекта, можете посмотреть мой доклад с [TechLeadConf 2024]({% post_url 2024/12/2024-12-20-data-flow-speeding-up %}). Там я рассказал про техники ускорения потоков данных и более подробно раскрыл детали этой предметной области. Эта статья является текстовым вариантом моего доклада с [TechLeadConf 2025]({% post_url 2025/06/2025-06-05-techleadconf-x-2025-presentation %}).

## Ненадёжный код

Для начала определимся, что же считать ненадёжным кодом? На самом деле ответ зависит от решаемой задачи, правил и процессов, принятых в компании.

* Код, который не прошел CI, review и т.п.
* Код из ненадёжного или неизвестного источника.
* Закрытый (проприетарный) код.
* Код, содержащий уязвимости.
* Код, использующий запрещенные функции.
* Любой код, который написал коллега. :)
* И т.д.

Чтобы отделять код разрабатываемого приложения от ненадёжного, первый буду называть кодом приложения, а второй — ненадёжным или внешним кодом. Необходимость запуска ненадёжного кода в некоторых случаях буду называть задачей.

## Уровни изоляции кода

Можно выделить три варианта запуска внешнего кода — три уровня изоляции. Каждый следующий увеличивает дистанцию между кодом приложения и запускаемым кодом. Чем выше уровень изоляции, тем меньше вероятность, что запускаемый код нанесет вред приложению и системе.

### Уровень 1: Тот же процесс

Запуск внешнего кода в адресном пространстве процесса приложения.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-iso-level-1.png)

Такой способ определяет самый слабый уровень изоляции, можно сказать, что "нулевой", поскольку запущенный код теоретически имеет доступ ко всему тому, к чему имеет доступ код самого приложения.

Примером может служить использование интерпретаторов скриптов ([Rhino](https://github.com/mozilla/rhino), [IronPython](https://github.com/IronLanguages/ironpython3), [Jython](https://github.com/jython/jython) и т.п.), визуальных языков программирования (workflow-движков) или подключение модулей расширения (плагинов).

Способов защиты на этом уровне не так много. Пожалуй, самым эффективным является [самоизоляция]({% post_url 2025/03/2025-03-10-process-isolation %}) (self-sandboxing), при которой приложение делает самозапрет на доступ к некоторым ресурсам системы. Например, сразу после инициализации приложение может сделать самозапрет на доступ к файловой системе.

Дополнительно запускаемый код можно подвергать строгому (синтаксическому) анализу, запрещая использование определенных функций, модулей, пакетов и т.п. Некоторые интерпретаторы имеют точки расширения, которые позволяют контролировать процесс исполнения. Если такой возможности нет, можно воспользоваться одной из техник самоизоляции — [фильтрацией системных вызовов](https://www.kernel.org/doc/html/latest/userspace-api/seccomp_filter.html).

Что же касается плагинов, то они призваны расширять возможности приложения, поэтому их использование изначально не предполагает сильной изоляции. Здесь можно предложить усилить контроль взаимодействия на уровне контракта (API), используя который плагин интегрируется с приложением. В идеале, если плагины будут публиковаться в некоторый центральный репозиторий, которому вы доверяете и который может производить дополнительные проверки и тестирование до этапа запуска кода плагина.

Рассмотрение возможных подходов на данном уровне изоляции требует написания отдельной статьи, что я и постараюсь сделать в ближайшем будущем. А пока рассмотрим два других способа изоляции.

### Уровень 2: отдельный процесс

Запуск внешнего кода на той же машине, но в отдельном процессе ОС.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-iso-level-2.png)

Поскольку и приложение, и внешний код взаимодействуют в рамках одного узла, используя локальные ресурсы ОС (оперативная память, файловая система и т.п.), скорость межпроцессного взаимодействия очень высокая.

Этот уровень изоляции предполагает использование широкого арсенала возможностей. Как минимум, внешний код может быть запущен от имени менее привилегированного пользователя, с ограниченным доступом к ресурсам ОС. Более сильные способы изоляции процесса предполагают ограничение ресурсов с помощью средств ОС или инструментов контейнеризации. Однако, чем сильнее контроль, тем больше накладных расходов на запуск и исполнение процесса, что при решении некоторых задач неприемлемо дорого или неоправданно сложно.

Вопрос изоляции ресурсов процесса ОС будет подробно рассмотрен далее.

### Уровень 3: отдельная машина

Запуск внешнего кода на отдельной машине — песочнице.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-iso-level-3.png)

Это максимальный уровень изоляции из всех возможных. Здесь появляется возможность ограничить ресурсы самой песочницы (CPU, память, дисковое пространство, доступ к сети и т.п.). Если в результате исполнения внешнего кода песочница выйдет из строя, приложение продолжит свою работу. Таким образом, увеличивается [устойчивость системы]({% post_url 2024/12/2024-12-30-reliability-strength-stability %}) к непредвиденным сбоям.

Самый главный недостаток этого подхода — необходимость сетевого взаимодействия между узлом, на котором работает приложение, и песочницей. Передача входных данных в песочницу, запуск процесса внутри песочницы, ожидание окончания его исполнения, получение выходных данных — всё это сетевые обращения. Это существенно замедляет процесс исполнения, а само взаимодействие подвержено сетевым сбоям, что ведёт к нестабильности системы и получаемых результатов.

## Использование песочницы

Предположим, требуется максимальный уровень изоляции ненадёжного кода, следовательно, нужно остановиться на варианте запуска на отдельной машине. Если так, то для принятия последующих архитектурных решений нужно ответить на следующую пару вопросов.

### Пересоздание или переиспользование песочницы

Песочницу требуется пересоздавать перед исполнением каждой задачи, если каждая требует особенное окружение (например, определенную версию ОС, пакетов или ресурсов) или идентичность этого окружения (для обеспечения стабильности получаемых результатов). Схожие вопросы возникают, например, при проведении интеграционного тестирования: каждому тесту нужны свои предустановки или тесты меняют состояние стенда так, что их повторный запуск становится невозможным.

Переиспользование песочницы становится возможным, если задачи могут исполняться в одном окружении и не оказывают влияния друг на друга (предыдущая задача не портит результаты последующей). Продолжая аналогию с интеграционным тестированием: всем тестам нужны одинаковые предустановки, и тесты могут запускаться повторно на одном стенде, демонстрируя один и тот же результат.

Основным преимуществом пересоздания песочницы является стабильность получаемых результатов. К недостаткам относится медленный запуск и перерасход ресурсов. На пересоздание песочницы уходят десятки секунд или даже минут, следовательно, большая часть ресурсов будет тратиться именно на это. Существует множество техник ускорения пересоздания, благодаря которым можно сократить время запуска. Прежде всего сюда можно отнести backup/restore (snapshot песочницы, базы данных и т.п.). Также, если поток задач небольшой и ресурсы позволяют, можно попробовать организовать пул песочниц и создавать их заранее.

Ставка на переиспользование делается в случае, когда поток задач большой и нужно сократить время ожидания их запуска. При этом возрастает вероятность получения нестабильных результатов и, возможно, потребуется производить какую-то очистку окружения до или после исполнения очередной задачи.

### Последовательное или параллельное исполнение

Теперь осталось ответить на вопрос, как именно можно или нужно исполнять задачи: последовательно или параллельно. Последовательное исполнение требуется в следующих случаях:

* важен порядок следования и исполнения задач;
* задачам нужен эксклюзивный доступ к определенному ресурсу;
* задачи ёмкие и их совместное исполнение вызовет нехватку ресурсов;
* задачи могут мешать исполнению друг друга из-за борьбы за ресурсы.

Например, шаги установки и настройки ПО; шаги CI/CD-конвейера; рендеринг изображения на GPU; интенсивные вычисления. Все эти задачи, скорей всего, придётся исполнять последовательно.

В остальных случаях, не попадающих под вышеуказанные правила, допустимо параллельное исполнение. Яркой аналогией может служить одна из лучших практик в тестировании: тесты не должны оказывать влияние друг на друга, а порядок их запуска не должен иметь значения.

Последовательное исполнение обеспечивает стабильность получаемых результатов, однако приводит к низкой пропускной способности и дороговизне масштабирования (песочница обходится дороже процесса ОС). Параллельное исполнение, напротив, увеличивает пропускную способность системы и улучшает утилизацию ресурсов песочницы, но одновременно возрастает вероятность получения нестабильных результатов. Более того, при параллельном исполнении появляется шанс перегрузить песочницу или вывести её таким образом из строя, что приведет к увеличению времени исполнения всех запущенных задач или потере результатов их работы.

На практике было замечено, что при параллельном исполнении, несмотря на увеличение общей пропускной способности, время исполнения каждой отдельной задачи увеличивается. Если уровень параллелизма становится больше числа CPU-ядер, время исполнения начинает деградировать намного сильней.

## Управление песочницами

Допустим, переиспользование песочниц возможно. В таком случае необходимо определить способ управления ими. Можно выделить два подхода, основанные на принципах микросервисной архитектуры, но адаптированные к специфике рассматриваемой проблемы.

### Оркестрация

Оркестрация предполагает, что приложение совмещает две роли: оркестратор исполнения и оператор песочниц. Оркестратор координирует процесс исполнения кода: выбор подходящей песочницы, загрузка в неё входных данных, запуск удалённого процесса и ожидание окончания его исполнения, получение результатов его работы и т.п. Оператор, в свою очередь, отслеживает доступные песочницы и их состояние.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-orchestration.png)

Основное преимущество оркестрации в контексте решаемой проблемы — это её простота и ясность. Код легко читается и сосредоточен в одном месте. Однако у этого решения есть и недостатки. Рассмотрим их в порядке от простого к сложному.

* *Синхронное взаимодействие.* Так или иначе, для получения результата приложение вынуждено ожидать окончания исполнения задачи. Для продуктивного использования ресурсов приходится прибегать к техникам асинхронного программирования: пока задача исполняется, приложение будет занято полезной работой. Это малозаметный недостаток в языках со встроенной поддержкой концепции асинхронного программирования, но, например, в Java это может стать испытанием, поэтому приходится использовать различные инструменты и фреймворки. Для упрощения работы с асинхронным кодом в Java я создал небольшую вспомогательную библиотеку [asynchronizer](https://github.com/AlexMAS/asynchronizer), снабдив её подробной [документацией](https://github.com/AlexMAS/asynchronizer/blob/main/docs/README.ru.md). Надеюсь, что эта небольшая работа принесёт пользу не только мне.

* *Отслеживание доступности песочниц.* Поскольку хотелось бы, чтобы количество песочниц менялось в зависимости от нагрузки на систему, придётся отслеживать их доступность. Это прямая обязанность оператора песочниц, которую можно выделить в отдельный discovery-сервис (например, на базе [Netflix Eureka](https://github.com/spring-cloud/spring-cloud-netflix)) либо реализовать как часть приложения с использованием инфраструктурных механизмов (например, [Kubernetes API](https://github.com/fabric8io/kubernetes-client)). Важно отметить, что оператор песочниц не имеет отношения к бизнес-логике приложения, однако является неотъемлемой и важной частью решения с оркестрацией.

* *Отслеживание загруженности песочниц.* В идеале, если нагрузка на песочницы будет распределена равномерно, а вероятность их перегрузки будет незначительной. Иначе говоря, оркестратор исполнения должен выбрать подходящую [стратегию балансировки](https://samwho.dev/load-balancing/), основанную на состоянии песочниц, предоставляемых оператором. На практике наилучшую эффективность демонстрирует алгоритм "Least connections", с помощью которого можно выбирать наименее загруженные песочницы. Для этого достаточно вести учёт количества задач, исполняемых каждой песочницей. Конечно, это не серебряная пуля, а лишь частное наблюдение, поэтому в идеале предусмотреть несколько стратегий балансировки и выбрать наилучшую по результатам нагрузочного тестирования.

* *Неопределённость результата, если нет ответа от песочницы.* Песочница может быть недоступна по различным причинам, включая не только проблемы с сетью, но и падения песочницы по причине исполнения ненадёжного кода. К сожалению, в общем случае эта проблема не имеет решения, так как делать повторные запуски (retries) может быть опасно. Всё, что остаётся, это использовать таймауты и откладывать неуспешную задачу "на потом". Подобные ситуации формируют у пользователя чувство ненадёжности системы в целом.

Ещё один существенный минус, который стоит упомянуть, это возможный побочный эффект, возникающий при масштабировании системы и проявляющийся в виде перегрузки песочниц. Для наглядности рассмотрим конкретный пример.

Для отслеживания нагрузки на песочницы экземпляр приложения ориентируется на количество задач в каждой из доступных песочниц. Предположим, что было принято решение увеличить количество экземпляров приложения. Этот экземпляр исполняет 5 задач в двух песочницах (3 процесса в одной и 2 в другой). Известно, что каждая песочница может вынести максимум 4 параллельных задачи.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-orchestration-scale-1.png)

Добавив новый экземпляр приложения, оно не знает, сколько задач исполняет каждая песочница. Такая ситуация может произойти по разным причинам.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-orchestration-scale-2.png)

Вполне очевидно, что новый экземпляр приложения направит очередную задачу в первую попавшуюся песочницу, чем может спровоцировать её перегрузку. В итоге результат исполнения будет испорчен или потерян из-за падения песочницы.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-orchestration-scale-3.png)

В качестве решения можно предложить два способа, каждый из которых уменьшает вероятность возникновения перегрузок, но не избавляет от них.

* *Для контроля количества исполняемых задач в песочнице использовать распределённый счётчик* (например, на базе Redis). Проблема в том, что распределённый счётчик также имеет латентность и на момент запуска задач может выдать устаревшее значение. Кроме того, в системе появляется еще один инфраструктурный компонент, который не несёт бизнес-пользы.

* *Выделить каждому экземпляру приложения эксклюзивное подмножество песочниц.* Подобное решение существенно усложнит deployment-скрипты и процесс масштабирования, а также снизит степень утилизации выделенных ресурсов, ведь нет никаких гарантий того, что экземпляр приложения сможет хорошо нагрузить все выделенные ему песочницы.

Не исключаю, что при рассмотрении частных ситуаций могут появиться и другие решения, но они, очевидно, будут только усложнять и без того сложное решение. Именно по этой причине следует рассмотреть альтернативный вариант, который я назвал "самоорганизация".

> Кстати, после доклада на TechLeadConf 2025 мне задали интересный вопрос: *"Можно ли при балансировке нагрузки на песочницы учитывать не только количество исполняемых задач, но и процент загрузки CPU, памяти и прочих ресурсов?"* Если у кого-то возник такой же вопрос, то отвечу, что это не имеет смысла, поскольку ситуация в песочнице может поменяться мгновенно. Полученный практический опыт и нагрузочное тестирование показали, что простой подсчёт задач работает приемлемо эффективно.

### Самоорганизация

В микросервисной архитектуре подобный подход принято называть хореографией, однако чтобы не возникало неправильных ассоциаций, предлагаю использовать термин "самоорганизация" (от "самоорганизация песочниц").

Ключевой момент в архитектуре — это появление двух очередей: очередь задач на исполнение (Task Queue) и очередь результата их исполнения (Result Queue). Все поступающие задачи приложение направляет в первую очередь, а результаты из второй. Дополнительно появляется роль агента — микросервиса, который исполняется в рамках узла песочницы и координирует исполнение поступающих задач. Задачи по-прежнему исполняются в отдельном процессе ОС.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-self-coordination.png)

Основной недостаток самоорганизации — распределённый процесс обработки задач. Учитывая простоту алгоритма обработки, это не так существенно. Стоит отметить преимущества этой архитектуры.

* *Максимальная изоляция ненадёжного кода.* Ненадёжный код, как и в случае с оркестрацией, по-прежнему работает в песочнице, в рамках отдельного процесса ОС.
* *Скорость и стабильность взаимодействия.* Никаких проблем с сетью и неопределённостью результата из-за локальности взаимодействия между агентом и песочницей.
* *Контролируемая нагрузка на песочницы.* Агент, выступая в роли консюмера очереди задач, может точно контролировать степень параллелизма и выбирать новые задачи только тогда, когда он закончил обрабатывать предыдущие.
* *Минимум инфраструктурного кода.* Очереди избавляют от необходимости иметь оператор песочниц, отслеживать их состояние и осуществлять балансировку нагрузки. Нет проблем с неопределённостью результата вследствие падения песочницы, так как само по себе падение уже не является неопределённостью и легко идентифицируется в потоке ошибок.
* *Простота масштабирования.* Приложение и песочницы масштабируются независимо друг от друга без негативных побочных эффектов.

## Запуск процесса ОС

К запуску процесса ОС, в рамках которого будет исполняться ненадёжный код, нужно подойти с особой осторожностью. Здесь нужно ответить как минимум на три вопроса.

* *Как ограничить права доступа к ресурсам.* Самое простое решение — это запуск процесса от имени пользователя с ограниченными правами (на доступ к ресурсам ОС).
* *Как ограничить объем используемых ресурсов.* Для запускаемого процесса нужно определить доступные ресурсы и возможные действия. Варианты решения подробно рассмотрены ниже.
* *Как осуществлять анализ поведения и результатов исполнения.* Наличие и решение этой проблемы целиком и полностью зависит от специфики проекта. Здесь невозможно предложить универсального решения.

Рассмотрим варианты ограничения ресурсов процесса ОС.

> В данном разделе под ресурсами понимается не только CPU, память или подсистема ввода-вывода, но и возможности самой ОС, которыми может воспользоваться запускаемый код.

### Ограничение ресурсов процесса

Ограничение ресурсов процесса ОС — одна из самых непростых задач со множеством возможных решений, каждое из которых имеет уйму особенностей. Рассмотрим основные решения.

Ресурсы процесса могут быть ограничены на трех уровнях:

* *Лимиты узла.* Физические ограничения машины, на котором исполняется процесс. В частном случае можно говорить об инфраструктурных лимитах, определённых для (Docker/Kubernetes) контейнера.
* *Лимиты контейнера.* Программные лимиты, задаваемые выбранным инструментом контейнеризации (cgroup, Docker, [Bubblewrap]({% post_url 2025/03/2025-03-06-bubblewrap %}), [ProcessSandbox](https://github.com/AlexMAS/ProcessSandbox) и т.п.).
* *Лимиты процесса.* Программные лимиты, задаваемые средствами ОС. На этом уровне можно осуществлять гибкую настройку вариантов запуска и исполнения.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-resource-limits-1.png)

Можно использовать все три уровня лимитирования, либо какой-то определенный.

Между тем, важно отметить некоторые трудности, которые могут возникнуть при использовании инструментов контейнеризации. Например, для использования cgroup или Docker внутри Kubernetes-контейнера нужно эскалировать привилегии контейнера, что в общем случае небезопасно в контексте исполнения ненадёжного кода. Более того, практика показала, что легковесных rootless-средств, предоставляемых ОС, вполне достаточно, чтобы снять большую часть рисков. В частности, Linux API позволяет не только лимитировать CPU и память, но и блокировать доступ к некоторым возможностям самой ОС. Например, можно наложить фильтр, который запретит вызов определённых системных функций.

### Проблемы жёсткого лимитирования

Рассмотренные выше способы лимитирования задают жёсткие границы (hard limit), нарушение которых замедляет исполнение процесса, либо приводит к его принудительному завершению. При этом поведение наблюдаемого процесса и системы сильно варьируется в зависимости от того, какой лимит был превышен. Например, превышение по использованию CPU может привести к троттлингу (throttling), приостановке работы или принудительному завершению; превышение по использованию памяти заканчивается принудительным завершением со стороны ОС (OOM Killer) либо самостоятельным падением процесса (с ошибкой Out Of Memory).

Подобная вариативность осложняет *анализ поведения и результатов исполнения.* В этом случае можно использовать подход с "программной мягкой границей" (watchdog limit). Суть подхода заключается в запуске дополнительного следящего потока (или процесса) ОС, который контролирует поведение и расход ресурсов у наблюдаемого. Как только детектируется превышение одного из лимитов, производится принудительное завершение наблюдаемого процесса, но уже не со стороны ОС, а со стороны приложения.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-resource-limits-2.png)

Такой подход имеет несколько преимуществ.

* *Точное определение причин принудительного завершения.* Жёсткие лимиты можно задать чуть выше мягких, благодаря чему для исполняемого кода создаётся иллюзия отсутствия каких-либо лимитов. Между тем, если лимиты всё-таки нарушаются, процесс всё равно будет завершен (либо со стороны приложения, либо гарантированно со стороны ОС). Но подобный дополнительный контроль со стороны приложения оставляет для него гораздо больше шансов понять причину принудительного завершения наблюдаемого процесса.

* *Возможность гибкого лимитирования ресурсов.* Приложение (или агент), ответственное за запуск наблюдаемого процесса, может обратиться к средствам ОС (в частности, к Linux API) и гибко настроить параметры запуска и исполнения. Как минимум, жёстко определить лимиты по CPU и памяти; наложить ограничения на объем I/O; создать запрет на вызов некоторых системных функций (например, запрет использования сетевых операций или файловой системы) и т.п.

Такой подход я назвал watchdog и в целях иллюстрации реализовал его в виде .NET-библиотеки [ProcessSandbox](https://github.com/AlexMAS/ProcessSandbox). Помимо прочего, на странице проекта подробно рассмотрена проблематика контроля и анализа поведения процесса ОС со стороны прикладного кода.

Итоговая схема лимитирования может выглядеть так, как показано на рисунке ниже. Вместо тяжеловесных инструментов контейнеризации используется легковесный rootless-инструмент (watchdog) на базе средств ОС и только.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-resource-limits-3.png)

Для задач, где не нужен анализ поведения процесса и тонкая настройка лимитов, можно воспользоваться готовым инструментом — утилитой [Bubblewrap]({% post_url 2025/03/2025-03-06-bubblewrap %}).

## Операционная система

Исполнение ненадёжного кода неминуемо будет приводить к принудительному завершению запускаемых процессов. Когда процесс завершается, он переходит в статус "зомби" (zombie, defunct), а ОС оповещает все процессы, которые ожидали его завершения. В общем случае зомби-процессы, не имеющие ожидающего родительского процесса, сигнализируют об ошибках ПО и не потребляют ресурсы. Однако, находясь на учете у ОС, они расходуют лимит на количество PIDs (максимальное число процессов и потоков ОС). Таким образом, рано или поздно это приведет к *невозможности запуска нового процесса или потока.* Эту проблему очень трудно обнаружить и диагностировать. Более подробно об этом я писал в статье "[Борьба с зомби-процессами]({% post_url 2025/01/2025-01-14-zombie-processes %})".

Решение заключается в подмене init-процесса на такой, который будет удалять зомби сразу, как только они появляются. Можно воспользоваться готовым решением с открытым кодом — [tini](https://github.com/krallin/tini).

## Многоконтейнерные поды

Зная, что контейнеры одного Kubernetes-пода работают на одном и том же узле, можно попытаться решить проблему нестабильности сетевого взаимодействия приложения и песочницы, разместив их контейнеры в одном поде.

![](/assets/img/blog/2025/2025-06-16-untrusted-code-pod.png)

Несмотря на всю заманчивость данной идеи, она несёт ряд недостатков.

* *Плохая масштабируемость.* Соотношение "приложение-песочница" всегда один к одному. Однако не исключено, что в некоторых случаях это вполне приемлемо.
* *Плохая утилизация ресурсов.* Сможет ли приложение достаточно нагрузить песочницу, если количество песочниц будет в избытке; и наоборот, нужно ли столько же экземпляров приложения, сколько и песочниц.
* *Возможность перегрузки песочницы.* В распоряжении экземпляра приложения только одна песочница, которая может не справиться с потоком задач, обрабатываемых приложением.
* *Риск нарушить работоспособность приложения.* Выход песочницы из строя, скорей всего, приведет к перезапуску всего пода. Более того, если приложение и песочница обмениваются файлами через общий раздел ([shared volume](https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/)), это может стать уязвимым местом.

## Образ для песочницы

Основные моменты, которые следует учесть при создании (Docker) образов песочниц:

* Заменить init-процесс на [tini](https://github.com/krallin/tini), чтобы не превысить лимит по PIDs.
* Создать непривилегированного пользователя, ограничив ему права на доступ к ресурсам.
* Регулярно сканировать версии образов и пакетов на наличие уязвимостей.

## Инфраструктура исполнения

Основные моменты, которые следует учесть при настройке инфраструктуры исполнения:

* Обеспечить быстрый (пере)запуск песочниц. Нужно быть готовым к тому, что песочницы будут падать. Если речь идет о Kubernetes, то улучшить время запуска может подходящая настройка [Image Pull Policy](https://kubernetes.io/docs/concepts/containers/images/). При этом лучше не использовать тег `latest`, а указывать конкретную версию или хэш-код образа, чтобы не тратить время на попытки определения последней версии при каждом запуске.
* Установить приемлемый [лимит на PIDs](https://kubernetes.io/docs/concepts/policy/pid-limiting/). Необходимо контролировать число активных процессов в системе. Особо вредоносный код может попытаться создать очень много дочерних процессов, поэтому при отсутствии лимита на PIDs узел быстро будет выведен из строя. Важно отметить, что лимит на PIDs задаётся для пользователя, а не для запускаемого процесса. По этой причине он должен быть разумно большим, в том числе и потому, что в Kubernetes этот лимит разделяется между всеми контейнерами узла.
* Установить [лимиты на ресурсы узла](https://kubernetes.io/docs/concepts/policy/resource-quotas/). В Kubernetes для каждого контейнера нужно указать, как минимум, лимиты по CPU и памяти. Значения лимитов лучше всего определить в ходе нагрузочного тестирования или путём сбора метрик приложения.

## Заключение

Как можно заметить, задача исполнения ненадёжного кода всегда решается в комплексе, начиная с анализа, продолжая разработкой и заканчивая вопросами уровня DevOps. Думаю, что многие техники и инструменты применимы и к коду самого приложения, ведь наш код использует сторонние зависимости, надёжность которых мы не можем гарантировать.

Я постарался показать последовательность шагов по направлению к целевой архитектуре, которая будет отвечать требованиям бизнеса и справляться с ненадёжным кодом. На каждом шаге задаются вопросы, ответы на которые уточняют вектор развития. И, как видно, на каждом из шагов приходится идти на какие-то компромиссы, часто разменивая безопасность и стабильность на производительность и наоборот.
